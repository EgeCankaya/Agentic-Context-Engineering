# Agentic Context Engineering (ACE)

[![Release](https://img.shields.io/github/v/release/EgeCankaya/Agentic-Context-Engineering)](https://img.shields.io/github/v/release/EgeCankaya/Agentic-Context-Engineering)
[![Build status](https://img.shields.io/github/actions/workflow/status/EgeCankaya/Agentic-Context-Engineering/main.yml?branch=main)](https://github.com/EgeCankaya/Agentic-Context-Engineering/actions/workflows/main.yml?query=branch%3Amain)
[![codecov](https://codecov.io/gh/EgeCankaya/Agentic-Context-Engineering/branch/main/graph/badge.svg)](https://codecov.io/gh/EgeCankaya/Agentic-Context-Engineering)
[![Commit activity](https://img.shields.io/github/commit-activity/m/EgeCankaya/Agentic-Context-Engineering)](https://github.com/EgeCankaya/Agentic-Context-Engineering)
[![License](https://img.shields.io/github/license/EgeCankaya/Agentic-Context-Engineering)](https://github.com/EgeCankaya/Agentic-Context-Engineering)

**Agentic Context Engineering (ACE)** â€” Evolving Contexts for Self-Improving Language Models

A framework that enables self-improvement of large language models through context evolution rather than model fine-tuning. The system iteratively improves performance by running a three-phase loop: **Generator â†’ Reflector â†’ Curator**.

## ğŸš€ Quick Start

### Prerequisites

- **GPU**: NVIDIA RTX 4070 Ti Super (16GB VRAM) or equivalent
- **RAM**: 32GB DDR5
- **Python**: 3.9+
- **CUDA**: Version 11.8 or higher

### Installation

1. **Install Ollama and pull Llama 3.1 8B:**
```bash
# Install Ollama (Windows)
winget install Ollama.Ollama

# Pull Llama 3.1 8B Instruct (~16GB download)
ollama pull llama3.1:8b-instruct-fp16

# Verify installation
ollama list
```

2. **Install ACE:**
```bash
git clone https://github.com/EgeCankaya/Agentic-Context-Engineering.git
cd Agentic-Context-Engineering
pip install -e .
```

3. **Verify setup:**
```bash
python verify_setup.py
```

### Basic Usage

```python
from agentic_context_engineering import ACERunner

# Initialize ACE system
runner = ACERunner()

# Run 5 iterations
results = runner.run_iterations(num_iterations=5)

# Check results
print(f"Completed {results['total_iterations']} iterations")
print(f"Final playbook version: {results['final_playbook'].version}")
```

### CLI Usage

```bash
# Run ACE iterations
python -m agentic_context_engineering.cli run --iterations 10

# Evaluate a playbook
python -m agentic_context_engineering.cli evaluate --playbook outputs/playbook_v1.5.0.yaml

# Compare two playbook versions
python -m agentic_context_engineering.cli diff --from-version 1.0.0 --to-version 1.5.0

# Check system health
python -m agentic_context_engineering.cli check-gpu
```

## ğŸ“– How It Works

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Evaluation Dataset                     â”‚
â”‚              (Dev / Iteration / Held-Out)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ACE Iteration Controller                    â”‚
â”‚              (ace_runner.py)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LangGraph Orchestrator                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚Generator â”‚ -> â”‚Reflector â”‚ -> â”‚ Curator  â”‚         â”‚
â”‚   â”‚  Agent   â”‚    â”‚  Agent   â”‚    â”‚  Agent   â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚               â”‚                â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                         â”‚                               â”‚
â”‚          All use Llama 3.1 8B (via Ollama)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Playbook Repository                         â”‚
â”‚   (Versioned Context + Git History)                     â”‚
â”‚   - system_instructions                                 â”‚
â”‚   - heuristics                                          â”‚
â”‚   - examples                                            â”‚
â”‚   - constraints                                         â”‚
â”‚   - metadata                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Evaluation & Visualization                      â”‚
â”‚   - Metrics Dashboard                                   â”‚
â”‚   - Before/After Comparison                             â”‚
â”‚   - Playbook Diff Viewer                                â”‚
â”‚   - VRAM Usage Tracking                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The ACE Loop

1. **Generator**: Produces model outputs using current playbook context
2. **Reflector**: Evaluates outputs and identifies improvement areas
3. **Curator**: Updates playbook with new heuristics and examples

### Key Features

| Feature | Description |
|---------|-------------|
| **Playbook Schema** | Structured, versioned representation of model context with Pydantic validation |
| **Local Inference** | Zero-cost operation using Llama 3.1 8B via Ollama |
| **Iteration Loop** | Automated generationâ†’reflectionâ†’curation cycles with convergence detection |
| **Evaluation System** | Multi-metric evaluation (BLEU, ROUGE, task-specific accuracy) |
| **Version Control** | Git-based playbook versioning with semantic versioning |
| **CLI Interface** | Comprehensive command-line interface for all operations |

## ğŸ¯ Performance Benchmarks

### Hardware Requirements
- **GPU**: RTX 4070 Ti Super (16GB VRAM)
- **VRAM Usage**: ~9GB
- **Inference Speed**: 40-50 tokens/sec
- **Full Iteration**: 2-3 minutes

### Expected Improvements
- **Task Accuracy**: â‰¥15% improvement over baseline
- **Response Coherence**: â‰¥10% BLEU/ROUGE increase
- **Convergence**: Plateau within 10 iterations
- **System Reliability**: 100% iteration completion

## ğŸ“ Project Structure

```
agentic_context_engineering/
â”œâ”€â”€ agents/              # Generator, Reflector, Curator agents
â”œâ”€â”€ playbook_schema/     # Pydantic models + base template
â”œâ”€â”€ runners/             # ACE iteration orchestrator
â”œâ”€â”€ utils/               # LLM client, metrics, versioning
â”œâ”€â”€ eval/                # Evaluation dataset + evaluator
â”œâ”€â”€ configs/             # YAML configurations
â”œâ”€â”€ cli.py               # Command-line interface
â””â”€â”€ tests/               # Unit and integration tests
```

## ğŸ”§ Configuration

### Default Configuration (`configs/default.yaml`)

```yaml
llm:
  provider: "ollama"
  model: "llama3.1:8b-instruct-fp16"
  base_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 2000

ace:
  max_iterations: 10
  convergence_threshold: 0.05
  reflection_batch_size: 3
  early_stopping_patience: 2

evaluation:
  metrics: ["bleu", "rouge", "exact_match", "semantic_similarity"]
  holdout_ratio: 0.2
```

## ğŸ“Š Example Results

### Before ACE (Baseline)
**Input:** "How do I handle retries in Python API calls?"

**Output:**
```
You can use a try-except block to catch exceptions and retry the API call.
```

### After ACE (Iteration 8)
**Input:** "How do I handle retries in Python API calls?"

**Output:**
```
Use the `tenacity` library for robust retry logic with exponential backoff:

```python
from tenacity import retry, stop_after_attempt, wait_exponential
import requests

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def call_api():
    response = requests.get("https://api.example.com/data")
    response.raise_for_status()
    return response.json()
```

This will retry up to 3 times with exponential backoff (2s, 4s, 8s).

**Documentation:** https://tenacity.readthedocs.io/
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=agentic_context_engineering --cov-report=html

# Run specific test categories
pytest tests/test_playbook.py -v
pytest tests/test_llm_client.py -v
```

## ğŸ“š Documentation

- **API Reference**: [Documentation](https://EgeCankaya.github.io/Agentic-Context-Engineering/)
- **Paper**: [Agentic Context Engineering: Evolving Contexts for Selfâ€‘Improving Language Models](https://arxiv.org/abs/2510.04618)
- **Ollama Setup**: [Ollama Documentation](https://ollama.com/docs)
- **Integration Guide**: `docs/integration_guide.md`
- **Project 2 Checklist**: `docs/project2_integration_checklist.md`

## ğŸ§© Using ACE in Your Project

1. **Install the package**
   ```bash
   pip install "agentic-context-engineering @ git+https://github.com/EgeCankaya/Agentic-Context-Engineering.git"
   ```
2. **Load the latest playbook**
   ```python
   from agentic_context_engineering import Playbook

   playbook = Playbook.from_yaml("outputs/playbook_v1.2.0.yaml")
   bundle = playbook.export_context_for_rag({"course": 12})
   ```
3. **Generate answers with retrieved context**
   ```python
   from agentic_context_engineering import LLMClient
   from agentic_context_engineering.agents import Generator

   llm_client = LLMClient()
   generator = Generator(llm_client)
   response = generator.generate_with_context(question, playbook, retrieved_docs)
   citations = generator.extract_citations(response, retrieved_docs)
   ```
4. **Log conversations for ACE feedback**
   ```python
   from agentic_context_engineering import ConversationLogger

   logger = ConversationLogger(output_dir="outputs/conversations")
   session_id = logger.start_session(user_id="student")
   logger.log_turn(session_id, question, response, retrieved_docs)
   dataset = logger.export_for_ace()
   ```
5. **Run improvement cycles**
   ```python
   from agentic_context_engineering import ACERunner

   runner = ACERunner()
   runner.run_iterations(num_iterations=3, tasks=[item["input"] for item in dataset])
   ```

ğŸ“ Check `examples/external_rag_integration/` for a runnable demo that ties these steps together (simple RAG bot, conversation logger, and ACE trigger script).

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Research Foundation**: Based on the paper "Agentic Context Engineering: Evolving Contexts for Selfâ€‘Improving Language Models"
- **Model**: Llama 3.1 8B Instruct by Meta AI
- **Framework**: LangGraph for agent orchestration
- **Local Inference**: Ollama for zero-cost operation

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/EgeCankaya/Agentic-Context-Engineering/issues)
- **Discussions**: [GitHub Discussions](https://github.com/EgeCankaya/Agentic-Context-Engineering/discussions)
- **Email**: egemencankaya14@gmail.com

---

**Ready to evolve your LLM contexts?** ğŸš€

```bash
python verify_setup.py  # Check your system
python -m agentic_context_engineering.cli run --iterations 5  # Start iterating!
```
